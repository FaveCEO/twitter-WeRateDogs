{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reporting: wrangle_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: Files used for this analysis are support materials provided by Udacity as elevated access to Twitter's API was not granted.*\n",
    "\n",
    "Required libraries for wrangling are\n",
    "- pandas\n",
    "- numpy\n",
    "- requests\n",
    "- os\n",
    "- json\n",
    "\n",
    "Twitter_archive_enhanced csv which is the first file that was cleaned in this analysis, was read into the notebook using pandas read_csv. This dataset had issues such as\n",
    "- Columns which are not very necessary for this analysis\n",
    "- Missing values\n",
    "- Wrong datatype of tweet_id\n",
    "- Timestamp which has a wrong datatype and has both date and time combine together\n",
    "- Doggo, floofer, pupper and puppo columns\n",
    "- Source of the tweet given in url format\n",
    "- Contains information of retweets which is not needed for this analysis\n",
    "\n",
    "The issues were resolved as follows:\n",
    "1) Columns such as in_reply_to_status_id, in_reply_to_user_id, retweeted_status_id, retweeted_status_user_id columns, retweeted_status_timestamp, expanded_urls which are not important for this were dropped by creating a function drop_col which makes use of the <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html\">drop function</a>.\n",
    "```\n",
    "def drop_col(df, col_name, axis):\n",
    "    result = df.drop(col_name, axis, inplace=True)\n",
    "    return result\n",
    "```\n",
    "\n",
    "2) The missing values(NaN) which was mainly in the retweeted_status_id and retweeted_status_timestamp was taken care of by filling the null values with 'Not available' using <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.fillna.html\"> fillna</a>. \n",
    "\n",
    "3) The datatype of tweet_id though is numbers should not be in int but a string, tweet_id is more like the identification of tweet just like a name. A function datatype_converter was created\n",
    "```\n",
    "def datatype_converter(df, col_name, datatype):\n",
    "    df[col_name] = df[col_name].astype(datatype)\n",
    "    return (df[col_name])\n",
    "```\n",
    "4) The timestamp column having both the date and time combined should be split into different columns and this is done by using the <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.to_datetime.html\">pd.to_datetime</a> and in the process, the timestamp column is dropped as it is no longer needed. When this split happens both the date and time are in object(string) datatype but the date should be in datetime, this conversion was achieved still using the pd.to_datetime.\n",
    "\n",
    "5) Doggo, floofer, pupper and puppo columns were combined into one column(dog_stage) using <a href= \"https://stackoverflow.com/questions/50276129/how-to-combine-multiple-columns-in-a-pandas-dataframe-by-using-apply\">apply</a>. The doggo, floofer, pupper and puppo columns were dropped, instead of the names used to show the dog_stage, boolean values are used to know which dog_stage belongs to a particular dog and this is done by using a <a href=\"https://stackoverflow.com/questions/63538648/how-to-create-element-by-element-comparison-matrix-in-pandas\">comparison matrix</a>. So this returned true or false for wherever each dog stage exist. The column None are dogs whose dog stage was not specified\n",
    "\n",
    "6) Exact source of tweet was extracted using <a href= \"https://regexone.com/\">regex</a>\n",
    "\n",
    "7) To drop retweeted data the dataframe was queried for retweeeted_status_id that do not show \"Not Available\" because retweeted data will definitely have a status id when this was gotten, the index of the retweets were gotten and dropped alongside the column, this allows for every information about retweets to be dropped unlike just dropping the column alone. The retweeted_status_timestamp was also dropped.\n",
    "\n",
    "Next file that was imported is the image_predictions.tsv with the provided url using <a href=\"https://pypi.org/project/requests/\">python requests</a> after which it was read using pandas read_csv using \\t as a sep(this is because it is tab separated and not comma separated). The issues in this dataset include:\n",
    "- tweet_id being an integer instead of a string\n",
    "- contains prediction that are not dogs\n",
    "- from the prediction dogs fall into more than one type of breed.\n",
    "- some dog breed names had underscore and some started with lowercase while some started with upper case\n",
    "\n",
    "The issues were resolved as follows:\n",
    "Apply was used again to combine the p1_dog, p2_dog and p3_dog column in a dog_prediction column and rows which had all False were dropped as it was certain from the neural network prediction that there were no dogs in the image while the prediction which gave more than one prediction of dog was resolved by selecting the p1_dog, p2_dog or p3_dog which was True and if more than one appeared to be True, the one with the higher confidence level is selected and this is possible by using <a href=\"https://numpy.org/doc/stable/reference/generated/numpy.select.html\">np.select</a>.\n",
    "Underscore and lowercase issue was resolved using <a href=\"https://stackoverflow.com/questions/9452108/how-to-use-string-replace-in-python-3-x\">replace</a> and <a href=\"https://www.geeksforgeeks.org/string-capitalize-python/#:~:text=In%20Python%2C%20the%20capitalize(),string_name.\">capitalize</a> respectively\n",
    "\n",
    "The last file which is the tweet_json.txt was read using python i/o and the selected columns were the id,favorite count and retweet count. This file did not have so much issues aside the id having a different name unlike other files which had tweet_id so this was resloved by renaming the id and also changing the datatype to string\n",
    "\n",
    "After working on the files individually, the files were merged into one dataframe on the tweet_id and stored in csv file named twitter_archive_master.csv.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
